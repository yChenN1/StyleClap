#!/bin/bash
#SBATCH --job-name=clap_finetune
#SBATCH --output=./%x_%j.out
#SBATCH --error=./%x_%j.err
#SBATCH --partition=rtx8000
#SBATCH --gpus=2
#SBATCH --nodes=1
#SBATCH --ntasks-per-node=2
#SBATCH --mem=100G
#SBATCH --time=03-00:00:00 

source ~/.bashrc
conda activate vevo


cd /mnt/fast/nobackup/users/yc01815/code/CLAP/src/laion_clap

mkdir -p slurm_logs

WANDB_MODE=online torchrun --nproc-per-node=2 --master-port=21316 -- \
  -m training.main \
  --save-frequency 5 \
  --save-top-performance 3 \
  --save-most-recent \
  --dataset-type="toy" \
  --precision="fp32" \
  --batch-size=96 \
  --lr=1e-5 \
  --wd=0.0 \
  --epochs=45 \
  --no-eval \
  --workers=6 \
  --use-bn-sync \
  --amodel HTSAT-base \
  --tmodel roberta \
  --warmup 1000 \
  --train-data '/mnt/fast/nobackup/scratch4weeks/yc01815/llasa/dataset/VST_chunks/*.parquet'\
  --val-data '/mnt/fast/nobackup/scratch4weeks/yc01815/llasa/dataset/VST_chunks_valid/chunk_valid.parquet' \
  --report-to "wandb" \
  --wandb-notes "finetune-instruct" \
  --datasetnames "instruct" \
  --datasetinfos "train" "test" \
  --top-k-checkpoint-select-dataset="Clotho-test" \
  --top-k-checkpoint-select-metric="mAP@10" \
  --openai-model-cache-dir /mnt/fast/nobackup/scratch4weeks/yc01815/transformers_cache \
  --logs /mnt/fast/nobackup/scratch4weeks/yc01815/clap/finetuning/0721 \
  --seed 3407 \
  --gather-with-grad \
  --optimizer "adam" \
  --data-filling "repeatpad" \
  --data-truncating "rand_trunc" \
  --pretrained /mnt/fast/nobackup/scratch4weeks/yc01815/pretrain_models/HTSAT_music_speech_epoch_15_esc_89.25.pt 
